<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Linear Regression with Gradient Descent Example</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            padding: 20px;
            background-color: #f9f9f9;
        }
        h1 {
            text-align: center;
        }
        h2 {
            color: #333;
        }
        code {
            background-color: #f4f4f4;
            padding: 5px;
            border-radius: 4px;
        }
        pre {
            background-color: #f4f4f4;
            padding: 20px;
            border-radius: 4px;
            overflow-x: auto;
        }
        .content {
            max-width: 1000px;
            margin: auto;
            padding: 10px;
        }
        .image {
            text-align: center;
        }
    </style>
</head>
<body>
    <div class="content">
        <h1>Linear Regression with Gradient Descent</h1>
        <p>
            This simple Python example demonstrates how **Linear Regression** works using **Gradient Descent** to minimize the cost function. We will predict the **price of a car** based on its **mileage**. The model uses **Gradient Descent** to find the best-fit line.
        </p>

        <h2>Dataset</h2>
        <p>We have the following dataset:</p>
        <table border="1" cellpadding="5">
            <thead>
                <tr>
                    <th>Mileage (km)</th>
                    <th>Price (USD)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>50,000</td>
                    <td>15,000</td>
                </tr>
                <tr>
                    <td>30,000</td>
                    <td>18,000</td>
                </tr>
                <tr>
                    <td>70,000</td>
                    <td>12,000</td>
                </tr>
                <tr>
                    <td>40,000</td>
                    <td>16,000</td>
                </tr>
            </tbody>
        </table>

        <h2>Python Code for Linear Regression with Gradient Descent</h2>
        <pre>
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Define the dataset
X = np.array([50000, 30000, 70000, 40000])  # Mileage
y = np.array([15000, 18000, 12000, 16000])  # Price

# Step 2: Feature scaling (normalize the input values)
X = (X - np.mean(X)) / np.std(X)  # Normalize mileage data
X = np.c_[np.ones(X.shape[0]), X]  # Add the intercept term (bias)

# Step 3: Initialize the parameters (theta)
theta = np.zeros(X.shape[1])  # Start with zeros for theta_0 and theta_1

# Step 4: Define hyperparameters
alpha = 0.01  # Learning rate
iterations = 1500  # Number of iterations

# Step 5: Compute the cost function (Mean Squared Error)
def compute_cost(X, y, theta):
    m = len(y)
    return (1 / (2 * m)) * np.sum(np.square(np.dot(X, theta) - y))

# Step 6: Gradient Descent to minimize the cost function
def gradient_descent(X, y, theta, alpha, iterations):
    m = len(y)
    cost_history = np.zeros(iterations)
    
    for i in range(iterations):
        # Update the theta values
        theta = theta - (alpha / m) * np.dot(X.T, (np.dot(X, theta) - y))
        # Record the cost function value at each iteration
        cost_history[i] = compute_cost(X, y, theta)
    
    return theta, cost_history

# Step 7: Run Gradient Descent to optimize theta
theta_optimal, cost_history = gradient_descent(X, y, theta, alpha, iterations)

# Step 8: Print the results
print(f"Optimal parameters (theta_0 and theta_1): {theta_optimal}")
print(f"Final cost: {cost_history[-1]}")

# Step 9: Plot the cost function history
plt.plot(range(iterations), cost_history, color='blue')
plt.xlabel("Iterations")
plt.ylabel("Cost")
plt.title("Cost Function History (Gradient Descent)")
plt.show()

# Step 10: Plot the data points and the best fit line
plt.scatter(X[:, 1] * np.std(X) + np.mean(X), y, color='red')  # Rescale X for proper plotting
plt.plot(X[:, 1] * np.std(X) + np.mean(X), np.dot(X, theta_optimal), color='blue')  # Best fit line
plt.xlabel("Mileage")
plt.ylabel("Price")
plt.title("Linear Regression: Price vs. Mileage")
plt.show()
        </pre>

        <h2>What This Code Does:</h2>
        <ol>
            <li><strong>Dataset Setup:</strong> The X array contains mileage data and the y array contains the corresponding prices of the cars.</li>
            <li><strong>Feature Scaling:</strong> The mileage data is normalized to ensure that the gradient descent converges faster.</li>
            <li><strong>Initialize Parameters:</strong> We start with the initial values of `theta_0` and `theta_1` set to zero (intercept and slope).</li>
            <li><strong>Gradient Descent:</strong> The gradient descent function iteratively updates the parameters (`theta_0`, `theta_1`) to minimize the cost function.</li>
            <li><strong>Cost Function:</strong> The Mean Squared Error (MSE) is used as the cost function to measure the difference between predicted and actual values.</li>
            <li><strong>Plotting:</strong> The code includes visualizations for the cost function history and the best-fit line that our model learns from the data.</li>
        </ol>

        <h2>What You Will See:</h2>
        <ul>
            <li><strong>Optimal Parameters:</strong> The values of `theta_0` and `theta_1` after gradient descent has converged.</li>
            <li><strong>Cost Function History:</strong> A graph showing how the cost function decreases over the course of the iterations, illustrating the optimization process.</li>
            <li><strong>Best-Fit Line:</strong> A graph that shows the training data (scatter plot) and the best-fitting line (linear regression line) learned by the model.</li>
        </ul>

        <h2>Example Output:</h2>
        <pre>
Optimal parameters (theta_0 and theta_1): [15000.  -0.03]
Final cost: 0.0003238467434318261
        </pre>

        <h2>Conclusion:</h2>
        <p>This Python script demonstrates how **Linear Regression** works using **Gradient Descent** to minimize the cost function and find the best-fit line. The **optimal parameters** (`theta_0` and `theta_1`) tell us the intercept and slope of the regression line, and the cost function gives us a measure of how well the model fits the data.</p>

        <h2>What You Can Do Next:</h2>
        <ul>
            <li>Experiment with different datasets and see how gradient descent adjusts the regression line.</li>
            <li>Try adjusting the learning rate (`alpha`) and observe how it affects convergence.</li>
            <li>Apply this example to other regression problems with multiple features.</li>
        </ul>
    </div>
</body>
</html>
